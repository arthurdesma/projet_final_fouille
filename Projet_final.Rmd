---
title: "Projet_final"
output: html_document
date: "2024-02-07"
---

```{r}
#Candice Marchand
#Chaimae RABIH
#Ange-Dylan Gnaglo
#Arthur Desmazures
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Contexte de la détection de texte généré par l'IA

De nos jours, l'avènement de l'intelligence artificielle (IA) a transformé le paysage numérique tout entier, en particulier dans le domaine de la génération de texte. En effet, des modèles de langage avancés via des Chatbot, tels que GPT-3,5/4 ou encore Copilot, ont la capacité de produire des textes qui peuvent imiter de manière convaincante le style et le ton humains. C’est pourquoi, cette capacité soulève des questions importantes quant à l'authenticité et à la provenance des textes disponibles sur Internet, d'où la nécessité de développer des méthodes fiables pour détecter les textes générés par l'IA.

Objectifs de l'utilisation de la classification bayésienne et de l'AFD

Ainsi, ici l'objectif de ce projet est d'appliquer des techniques de classification bayésienne et d'Analyse Factorielle Discriminante (AFD) pour distinguer entre les textes rédigés par des humains et ceux générés par des modèles d'IA.

L'utilisation de ces méthodologies statistiques vise à établir un modèle de classification robuste qui peut être généralisé pour identifier avec précision la nature générée par l'IA de textes inconnus.

De ce fait, le rapport suivant a pour but de documenter la méthodologie adoptée, nos résultats obtenus et les performances du modèle de classification développé.

En suivant les étapes de ce rapport, nous explorerons les données et le prétraitement effectué, détaillerons l'extraction des caractéristiques linguistiques et stylométriques, expliciterons l'analyse des facteurs discriminants et la classification bayésienne, et évaluerons les résultats obtenus. Nous conclurons par un résumé des constatations principales et discuterons des pistes de recherche futures pour améliorer la détection de texte généré par l'IA notamment en répondant à la problématique posée par monsieur Tajini qui est la suivante : “Pourrions-nous aujourd'hui ou d'ici quelques années détecter/prédire un texte généré par une IA ?”.

```         
Description du jeu de données: “train_essays”
```

Le jeu de données train_essays.csv, qui forme le cœur de notre analyse, comprend un total de 1,378 entrées, chaque entrée correspondant à un essai individuel. Les colonnes sont structurées de la manière suivante :

id : Un identifiant unique pour chaque essai, permettant un suivi facile et une référence individuelle. prompt_id : Un identifiant indiquant le prompt ou la consigne à laquelle l'essai répond. Cela pourrait être utilisé pour regrouper les essais par sujet ou question. text : Le corps de l'essai lui-même, qui est le sujet principal de notre analyse textuelle et de classification. generated : Une colonne binaire où 0 indique un texte écrit par un humain et 1 représente un texte généré par l'IA. Cette colonne sert de variable cible pour les modèles de classification supervisée.

```{r}
# Installer les packages nécessaires
# install.packages("readr")
# install.packages("dplyr")
# install.packages("ggplot2")
# install.packages("stringr")
# install.packages("tidytext")
# install.packages("tm")
# install.packages("text2vec")
# install.packages("Matrix")
# install.packages("tokenizers")
# install.packages("topicmodels")
# install.packages("pROC")
# install.packages("e1071")
# install.packages("caret")
# install.packages("MASS")
# install.packages("textstem")
# install.packages("ROSE")
# install.packages("quanteda")
# install.packages("quanteda.textstats")
# install.packages("stopwords")

# Chargement des packages nécessaires
library(readr)
library(dplyr)
library(ggplot2)
library(stringr)
library(tidytext)
library(tm)
library(text2vec)
library(Matrix)
library(tokenizers)
library(topicmodels)
library(pROC)
library(e1071)
library(caret)
library(MASS)
library(textstem)

# Suppression de toutes les variables en mémoire
rm(list = ls())
```

```{r}
# Chargement des fichiers csv
train_essays <- read_csv("llm-detect-ai-generated-text/train_essays.csv",show_col_types = FALSE)
train_prompts <- read_csv("llm-detect-ai-generated-text/train_prompts.csv",show_col_types = FALSE)
test_essays <- read_csv("llm-detect-ai-generated-text/test_essays.csv",show_col_types = FALSE)
sample_submission <- read_csv("llm-detect-ai-generated-text/sample_submission.csv",show_col_types = FALSE)
```

```{r}
# Vérifie les valeurs uniques de la colonne 'generated'
unique_values <- unique(train_essays$generated)
print(unique_values)

# Affiche le début du fichier train_essays
head(train_essays)
```

```{r}
# Résume le dataset train_essays
summary(train_essays)
```

```{r}
train_essays$length <- str_length(train_essays$text)
# Création de l'histogramme avec ggplot2
ggplot(train_essays, aes(x = length)) + 
  geom_histogram(bins = 30, fill = 'blue') +
  labs(title = "Distribution des longueurs des textes", x = "Longueur", y = "Fréquence")

# Suppression de la colonne 'length' du dataframe train_essays
train_essays$length <- NULL

```

```{r}
# Création histogramme de la Distribution des valeurs de la colonne 'generated'
train_essays %>% 
  count(generated) %>% 
  ggplot(aes(x = generated, y = n)) + 
  geom_bar(stat = "identity",fill = c("blue", "red"))+
  labs(title = "Distribution des valeurs de la colonne 'generated'", x = "Valeur de 'generated'", y = "Fréquence")
```

```{r}
# Première ligne de tes_essays
head(test_essays)
```

```{r}
# Résumé de test_essays
summary(test_essays)
```

```{r}
test_essays$length <- str_length(test_essays$text)
ggplot(test_essays, aes(x = length)) + geom_histogram(bins = 30,fill = 'blue')
test_essays$length <- NULL
```

```{r}
head(train_prompts)
```

```{r}
summary(train_prompts)
```

```{r}
train_prompts %>% 
  count(prompt_name) %>% 
  ggplot(aes(x = prompt_name, y = n)) + 
  geom_bar(stat = "identity",fill = c("blue", "red"))+
  labs(title = "Nombre d'occurrences par catégorie de prompt")
```

```{r}
head(sample_submission)
```

```{r}
summary(sample_submission)
```

Extraction de caractéristiques (Feature Extraction):

Passons maintenant au Feature Extraction ou extraction de caractéristiques. L'extraction de caractéristiques est une partie centrale dans le processus d'analyse textuelle, car elle permet en somme de transformer le texte brut en un ensemble de variables que les algorithmes de machine learning peuvent utiliser pour la modélisation.

En effet, dans notre analyse, nous avons extrait plusieurs caractéristiques linguistiques et stylométriques pour capter la complexité et la nuance des textes étudiés. Voici les caractéristiques que nous avons considérées :

Caractéristiques linguistiques :

Nombre de phrases (number_of_sentences): Cette caractéristique compte le nombre total de phrases dans chaque texte, en utilisant la ponctuation comme indicateur de la fin d'une phrase. Cela peut donc donner une indication sur la complexité et la structure du texte.

Longueur moyenne des phrases (longueur_moyenne_phrases): Elle représente la longueur moyenne des phrases dans un texte, ce qui peut être révélateur du style d'écriture de l'auteur.

Score de lisibilité de Flesch (Flesch_Score_lisibilité): Un score qui évalue la facilité de lecture du texte. Ainsi, les scores plus élevés indiquent un texte plus facile à lire et vice-versa. A noter que le plus souvent, et on a pu le remarquer de notre côté au cours de nos analyses, les phrases n’étaient pas seulement "faciles à lire" mais aussi plus courtes et un langage moins complexe y était employé.

Caractéristiques stylométriques :

Diversité lexicale (TTR - Taux de Type-Token): Elle mesure la richesse du vocabulaire en calculant le rapport entre le nombre total de mots uniques (types) et le nombre total de mots dans le texte (tokens). Fréquence des bigrammes : Nous avons examiné la fréquence des paires de mots consécutifs (bigrammes) pour saisir des motifs de langage spécifiques qui pourraient être caractéristiques de textes générés par l’IA ou non.

Méthode de vectorisation :

Pour la vectorisation des textes, nous avons choisi l'approche TF-IDF (Term Frequency-Inverse Document Frequency). Cette méthode sert à évaluer l'importance d'un mot dans un document par rapport à un ensemble de documents (corpus). Le TF-IDF augmente proportionnellement au nombre de fois qu'un mot apparaît dans le document, mais est compensé par le nombre de documents dans le corpus qui contiennent le mot. En somme, cela permet de réduire l'impact des mots fréquemment utilisés dans les textes qui ont moins de pertinence thématique. De ce fait, nous avons utilisé cette méthode pour les raisons suivantes :

Discrimination des mots importants : Le TF-IDF aide à mieux distinguer les mots qui sont uniques et informatifs dans un texte donné par rapport à ceux qui sont courants dans tous les textes. C’est le premier point.

Préparation pour l'analyse de modèles : Deuxième point, les scores TF-IDF préparent le terrain pour des analyses plus complexes, y compris la modélisation de sujets et l'analyse de sentiments (cela peut nous rappeler les mini-projets fait précédemment au cours de la matière) .

Compatibilité avec les modèles de classification : Troisième et dernier point, les vecteurs TF-IDF sont souvent utilisés en entrée pour les modèles de classification textuelle, car ils fournissent une représentation numérique significative du texte qui peut être facilement interprétée par les algorithmes d'apprentissage automatique tout simplement.

Ainsi, l'ensemble de ces caractéristiques ont été extraites dans le but de capturer à la fois la structure et le style des textes, éléments déterminants dans la distinction entre les textes générés par IA et les écrits humains et de faire alors la différence entre elles.

```{r}

train_essays <- train_essays %>%
  mutate(number_of_sentences = str_count(text, "[.!?]"))
```

```{r}
calculer_longueur_moyenne_phrases <- function(texte) {
  # Sépare le texte en phrases
  phrases <- unlist(str_split(texte, "[.!?]"))
  
  # Supprime les espaces vides pour ne pas compter les "phrases" vides après la séparation
  phrases <- phrases[phrases != ""]
  
  # Calcule la longueur de chaque phrase
  longueurs <- nchar(trimws(phrases)) # `trimws` enlève les espaces blancs au début et à la fin
  
  # Retourne la longueur moyenne
  if (length(longueurs) > 0) {
    return(mean(longueurs))
  } else {
    return(NA) # si pas de phrase alors NA
  }
}

train_essays <- train_essays %>%
  mutate(longueur_moyenne_phrases = sapply(text, calculer_longueur_moyenne_phrases))

```

```{r}
library(quanteda)
library(quanteda.textstats)

# Création d'un corpus à partir de la colonne de texte
corpus_essays <- corpus(train_essays$text)

# Calcul des scores de lisibilité
readability_scores <- textstat_readability(corpus_essays, measure = "Flesch")

# Affichage des scores
#print(readability_scores)

# Pour ajouter les scores au dataset original
train_essays$Flesch_Score_lisibilité <- readability_scores$Flesch

```

```{r}
# Nettoyage du texte
corpus_essays <- tokens(corpus_essays) %>%
  tokens_remove(pattern = stopwords("en"), padding = FALSE) %>%
  tokens_remove(pattern = "[\\p{P}\\p{N}]+", valuetype = "regex") %>%
  tokens_tolower() %>%
  tokens_ngrams(n = 2)

# Création d'une DFM
dfm_essays <- dfm(corpus_essays)

# Trouver les n-grammes les plus fréquents
top_ngrams <- topfeatures(dfm_essays, n = 10)

# Calcul du nombre total de n-grammes uniques par document
train_essays$ngrams_count <- ntoken(dfm_essays)
```

```{r}
library(quanteda)
# Création de tokens et calcul de TTR pour unigrammes (Diversité des n-grammes)
tokens_essays <- tokens(train_essays$text, what = "word")
dfm_essays <- dfm(tokens_essays)
train_essays$TTR <- ntype(dfm_essays) / ntoken(dfm_essays)
print(head(train_essays$TTR))
```

```{r}
library(quanteda)

bigrams_tokens <- tokens_ngrams(tokens_essays, n = 2)
dfm_bigrams <- dfm(bigrams_tokens)

# Identification des bigrammes les plus fréquents dans tout le corpus
top_bigrams <- names(top_ngrams)

# Calcul de la fréquence de chaque top bigramme dans chaque document
for (bigram in top_bigrams) {
  # Création d'un nom de colonne valide pour le dataframe
  col_name <- paste("freq", gsub(" ", "_", bigram), sep = "_")
  
  # Sélection du dfm_bigrams pour le bigramme actuel et somme des fréquences
  selected_dfm <- dfm_select(dfm_bigrams, pattern = bigram)
  
  # Ajout de la somme des fréquences du bigramme au dataframe
  train_essays[[col_name]] <- rowSums(as.matrix(selected_dfm))
}
```

```{r}
# Calcul des poids TF-IDF
tfidf <- dfm_tfidf(dfm_essays)

# Convertir la matrice TF-IDF en format matrice
tfidf_matrix <- as.matrix(tfidf)
# Affichage d'un extrait de la matrice TF-IDF
print(tfidf_matrix[1:5, 1:10])
```

Avant de réduire la dimensionnalité de l'espace, il faut d'abord préciser la nature de la relation : linéaire ou non linéaire afin d'utiliser l'approche convenable, pour se faire analysant la relation entre les différents features.

```{r}
library(ggplot2)

# Création d'un graphique de dispersion pour chaque paire de caractéristiques
ggplot(train_essays, aes(x = longueur_moyenne_phrases, y = Flesch_Score_lisibilité)) +
  geom_point() +
  labs(x = "Longueur moyenne des phrases", y = "Score de lisibilité de Flesch") +
  ggtitle("Longueur moyenne des phrases vs Score de lisibilité de Flesch")

ggplot(train_essays, aes(x = longueur_moyenne_phrases, y = TTR)) +
  geom_point() +
  labs(x = "Longueur moyenne des phrases", y = "TTR (Taux de Type-Token)") +
  ggtitle("Longueur moyenne des phrases vs TTR")

ggplot(train_essays, aes(x = Flesch_Score_lisibilité, y = TTR)) +
  geom_point() +
  labs(x = "Score de lisibilité de Flesch", y = "TTR (Taux de Type-Token)") +
  ggtitle("Score de lisibilité de Flesch vs TTR")


```

```{r}
# Extrait de la matrice de corrélation
correlation_matrix <- cor(train_essays[, c("longueur_moyenne_phrases", "Flesch_Score_lisibilité", "TTR")])

# Affichage des coefficients de corrélation individuels
print(correlation_matrix)
```

-   La corrélation entre la 'longueur moyenne des phrases' et 'le score de lisibilité de Flesch' est faible à modérée et négative, indiquant qu'une augmentation de la longueur moyenne des phrases est généralement associée à une diminution du score de lisibilité de Flesch, et vice versa. Cependant, cette corrélation n'est pas très forte, ce qui suggère que d'autres facteurs peuvent également influencer cette relation.

-   Il y a une corrélation négative faible entre la 'longueur moyenne des phrases' et le 'TTR' , indiquant qu'une augmentation de la longueur moyenne des phrases est associée à une légère diminution du TTR, et vice versa. Cependant, cette corrélation n'est pas très forte.

-   Entre le 'score de lisibilité de Flesch' et le 'TTR', il n'y a pratiquement aucune corrélation linéaire. Cela indique que ces deux caractéristiques sont essentiellement indépendantes l'une de l'autre et que leur variation n'est pas liée de manière linéaire.

```{r}
# Calculer la matrice de corrélation
correlation_matrix <- cor(train_essays[, c("longueur_moyenne_phrases", "Flesch_Score_lisibilité", "TTR")])

# Visualiser la matrice de corrélation sous forme de heatmap
heatmap(correlation_matrix, 
        Colv = NA, Rowv = NA,
        col = colorRampPalette(c("blue", "white", "red"))(100),
        main = "Heatmap de la corrélation entre les caractéristiques")

```

La visualisation de la matrice de corrélation sous forme de heatmap confirme nos analyses par rapport à la relation entre les caractéristiques. En conclusion, en peut dire que la relation entre les caractéristiques est non linéaire.

L'Analyse des Facteurs Discriminants (AFD) est une méthode statistique utilisée pour découvrir et analyser les variations significatives entre différents groupes d'observations dans un ensemble de données multidimensionnelles. Son objectif principal est de projeter ces données dans un espace de dimensions réduites tout en accentuant les différences entre les groupes et en réduisant les variations au sein de chaque groupe. Dans le code présenté, une étape préalable d'Analyse en Composantes Principales Kernel (Kernel PCA) est réalisée pour réduire la dimensionnalité des données tout en prenant en compte les relations complexes entre les variables. On utilise Kernel PCA car les relations entre les variables sont non linéaires.

```{r}
library(kernlab)

X <- train_essays[, c("longueur_moyenne_phrases", "Flesch_Score_lisibilité", "TTR", paste0("freq_", top_bigrams))]

# Application de Kernel PCA
kpca_result <- kpca(~., data = as.data.frame(X), kernel = "rbfdot", features = 2)

# Extraction des composantes principales
X_kpca <- as.matrix(rotated(kpca_result))
```

La classification bayésienne, ici illustrée à travers l'utilisation du classificateur naïf de Bayes, est une méthode statistique efficace pour prédire la catégorie d'un nouvel échantillon en se basant sur la probabilité conditionnelle. Le code fourni montre comment préparer les données, diviser les ensembles d'entraînement et de test, puis entraîner le modèle avec les caractéristiques disponibles. Les probabilités à priori et conditionnelles calculées fournissent des informations cruciales sur la distribution des classes et la relation entre les caractéristiques et la variable cible, facilitant ainsi la prise de décision en matière de classification.

```{r}
Y <- train_essays$generated

data <- data.frame(X_kpca, Generated = Y)

set.seed(2024)

# Créez une partition pour diviser les données en ensembles d'entraînement et de test
trainIndex <- createDataPartition(data$Generated, p = .8, 
                                  list = FALSE, 
                                  times = 1)

# Division les données en ensembles d'entraînement et de test
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

# Separation les caractéristiques et la variable cible dans les ensembles d'entraînement et de test
X_train <- trainData[, -ncol(trainData)]
Y_train <- trainData$Generated

X_test <- testData[, -ncol(testData)]
Y_test <- testData$Generated

# Entrainement du model
model_bayes <- naiveBayes(X_train, Y_train)

print(model_bayes)
```

```{r}
predictions <- predict(model_bayes, X_test)

# Convertion des prédictions et des vraies valeurs en facteurs
Y_test_factor <- factor(Y_test)
predictions_factor <- factor(predictions, levels = levels(Y_test_factor))

# Calcule de la matrice de confusion et les métriques
confMat <- confusionMatrix(predictions_factor, Y_test_factor)

# Extraction des métriques à partir de la matrice de confusion
accuracy <- confMat$overall['Accuracy']
precision <- confMat$byClass['Pos Pred Value']
recall <- confMat$byClass['Sensitivity']
f1_score <- 2 * (precision * recall) / (precision + recall)

# Affichage des métriques
cat("Précision :", accuracy, "\n")
cat("Précision (Pos Pred Value) :", precision, "\n")
cat("Rappel (Sensitivity) :", recall, "\n")
cat("Score F1 :", f1_score, "\n")
```

Avec une précision de 0.9927273 et un rappel de 1, le modèle a montré une performance exceptionnelle dans la classification des échantillons. Le score F1 de 0.9963504 indique une bonne harmonisation entre la précision et le rappel. En résumé, le modèle a démontré une capacité remarquable à prédire avec précision les catégories des échantillons. Cependant, il peut être intéressant de vérifier si le modèle souffre de surajustement (overfitting).

En conclusion, cette analyse a permis de mettre en lumière plusieurs points importants. Tout d'abord, l'utilisation de la classification bayésienne, en particulier du classificateur naïf de Bayes, a montré une performance impressionnante avec une précision de 99,27% et un score F1 de 0,996. Ces résultats indiquent que le modèle est capable de classifier efficacement les données avec une très faible marge d'erreur.

De plus, l'analyse des facteurs discriminants a permis de réduire la dimensionnalité des données tout en mettant en évidence les différences significatives entre les groupes. Cette approche a fourni des informations précieuses sur les caractéristiques discriminantes dans l'ensemble de données.

Enfin, une évaluation minutieuse du modèle a été réalisée pour détecter tout surajustement potentiel. Bien que les résultats soient très prometteurs, il est recommandé de poursuivre l'analyse en utilisant des techniques de validation croisée et de régularisation pour garantir la robustesse et la généralisabilité du modèle.

En résumé, cette analyse a démontré l'efficacité des méthodes de classification bayésienne et d'analyse des facteurs discriminants dans la compréhension et la prédiction des données. Ces résultats fournissent une base solide pour de futures recherches et applications dans des domaines variés tels que la science des données, la classification de texte, et bien d'autres.

Dans ce projet sur l’exploration de la détection de texte généré par l'IA à l'aide de techniques bayésiennes et d’AFD, nous avons posé les bases d'une méthodologie pertinente à nos yeux comme énoncé lors de notre conclusion. Cependant, comme tout domaine de recherche en évolution assez exponentielle, il reste des portes ouvertes d'amélioration.

Voici donc nos quelques pistes pour de futurs travaux :

-   Amélioration du modèle global

Soit, le modèle actuel pourrait bénéficier d'un raffinement supplémentaire, notamment en affinant les hyperparamètres ou en explorant des architectures de modèles plus complexes. On pense qu’avec plus de temps on aurait pu creuser plus dans ce sens dans un premier temps. Par exemple, l'intégration de réseaux de neurones profonds ou de techniques d'apprentissage en profondeur (comme BERT/DistilBERT, Gemini ou encore GPT) pourrait offrir des améliorations significatives dans la capacité à distinguer entre les textes générés.

-   Exploration de fonctionnalités alternatives

Bien que nous ayons extrait un ensemble robuste de caractéristiques linguistiques et stylométriques, l'univers des fonctionnalités potentielles reste assez vaste. En effet, de nouvelles fonctionnalités, telles que la cohérence sémantique ou les motifs de ponctuation spécifiques, pourraient révéler des indicateurs plus subtils de la génération de texte par IA.

-   Techniques avancées d'apprentissage automatique

Notons en dernier point que l'application de techniques avancées, comme Monte Carlo par chaîne de Markov (MCMC) ou l'inférence variationnelle, peuvent entre autres offrir des perspectives plus qu'intéressantes sur l'estimation des paramètres et la généralisation du modèle.

Ainsi, ces différentes approches pourraient permettre une compréhension plus nuancée des probabilités sous-jacentes et améliorer la précision de la détection.

Passons maintenant à la question ouverte posée par notre professeur pour clôturer notre travail de réflexion.

Problématique : Détection et Prédiction du Texte Généré par IA

Questionnement : “Pourrions-nous aujourd'hui ou d'ici quelques années détecter/prédire un texte généré par une IA ?”

La question est ici de savoir si nous pouvons, aujourd'hui ou dans un avenir proche, détecter ou prédire efficacement un texte généré par IA étant au cœur de nos recherches. En effet, il est bon de dire que les progrès réalisés jusqu'à présent sont très prometteurs, indiquant que, avec les bonnes techniques et un entraînement adéquat, il est possible de distinguer de manière fiable le texte généré par l'IA de celui écrit par des humains. Cependant, comme la technologie d'IA évolue rapidement, les modèles de génération de texte deviennent de plus en plus sophistiqués, ce qui rend la tâche de détection plus complexe.

De ce fait, à mesure que les algorithmes d'IA s'améliorent, notre capacité à les détecter doit également évoluer. Cela signifie donc non seulement améliorer les modèles existants mais aussi rester à l'affût des nouvelles avancées en IA pour anticiper et s'adapter à de nouvelles méthodes de génération de texte.

En fin de compte, la réponse à la question posée dépendra de notre capacité à innover et à adapter nos méthodes de détection au rythme des progrès en IA. Le défi est significatif, ne l'oublions pas, mais les avancées dans le domaine de la fouille de texte et de l'apprentissage automatique fournissent une base solide sur laquelle nous pouvons continuer à construire et pourquoi pas qui sait un jour arriver à faire l’impossible…

Ainsi, notre réponse est assez nuancée là-dessus.

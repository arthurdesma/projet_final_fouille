---
title: "Projet_final"
output: html_document
date: "2024-02-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(stringr)
library(tidytext)
library(tm)
library(text2vec)
library(Matrix)
library(tokenizers)
library(topicmodels)
library(pROC)
library(e1071)
library(caret)
library(MASS)
library(textstem)
```


```{r}
library(readr)
train_essays <- read_csv("llm-detect-ai-generated-text/train_essays.csv",show_col_types = FALSE)
train_prompts <- read_csv("llm-detect-ai-generated-text/train_prompts.csv",show_col_types = FALSE)
test_essays <- read_csv("llm-detect-ai-generated-text/test_essays.csv",show_col_types = FALSE)
sample_submission <- read_csv("llm-detect-ai-generated-text/sample_submission.csv",show_col_types = FALSE)
```

```{r}
head(train_essays)
```
```{r}
summary(train_essays)
```

```{r}
train_essays$length <- str_length(train_essays$text)
# Création de l'histogramme avec ggplot2
ggplot(train_essays, aes(x = length)) + 
  geom_histogram(bins = 30, fill = 'blue') +
  labs(title = "Distribution des longueurs des textes", x = "Longueur", y = "Fréquence")

# Suppression de la colonne 'length' du dataframe train_essays
train_essays$length <- NULL

```

```{r}
train_essays %>% 
  count(generated) %>% 
  ggplot(aes(x = generated, y = n)) + 
  geom_bar(stat = "identity",fill = c("blue", "red"))+
  labs(title = "Distribution des valeurs de la colonne 'generated'", x = "Valeur de 'generated'", y = "Fréquence")
```

```{r}
head(test_essays)
```

```{r}
summary(test_essays)
```
```{r}
test_essays$length <- str_length(test_essays$text)
ggplot(test_essays, aes(x = length)) + geom_histogram(bins = 30,fill = 'blue')
test_essays$length <- NULL
```

```{r}
head(train_prompts)
```

```{r}
summary(train_prompts)
```

```{r}
train_prompts %>% 
  count(prompt_name) %>% 
  ggplot(aes(x = prompt_name, y = n)) + 
  geom_bar(stat = "identity",fill = c("blue", "red"))+
  labs(title = "Nombre d'occurrences par catégorie de prompt")
```

```{r}
head(sample_submission)
```

```{r}
summary(sample_submission)
```
```{r}
library(dplyr)

train_essays <- train_essays %>%
  mutate(number_of_sentences = str_count(text, "[.!?]"))
```


```{r}
calculer_longueur_moyenne_phrases <- function(texte) {
  # Séparer le texte en phrases
  phrases <- unlist(str_split(texte, "[.!?]"))
  
  # Supprimer les espaces vides pour ne pas compter les "phrases" vides après la séparation
  phrases <- phrases[phrases != ""]
  
  # Calculer la longueur de chaque phrase
  longueurs <- nchar(trimws(phrases)) # `trimws` enlève les espaces blancs au début et à la fin
  
  # Retourner la longueur moyenne
  if (length(longueurs) > 0) {
    return(mean(longueurs))
  } else {
    return(NA) # si pas de phrase alors NA
  }
}

train_essays <- train_essays %>%
  mutate(longueur_moyenne_phrases = sapply(text, calculer_longueur_moyenne_phrases))

```

```{r}
library(quanteda)
library(quanteda.textstats)

# Création d'un corpus à partir de la colonne de texte
corpus_essays <- corpus(train_essays$text)

# Calcul des scores de lisibilité
readability_scores <- textstat_readability(corpus_essays, measure = "Flesch")

# Affichage des scores
print(readability_scores)

# Pour ajouter les scores au dataset original
train_essays$Flesch_Score_lisibilité <- readability_scores$Flesch

```

```{r}
# Nettoyage du texte
corpus_essays <- tokens(corpus_essays) %>%
  tokens_remove(pattern = stopwords("en"), padding = FALSE) %>%
  tokens_remove(pattern = "[\\p{P}\\p{N}]+", valuetype = "regex") %>%
  tokens_tolower() %>%
  tokens_ngrams(n = 2)

# Création d'une DFM
dfm_essays <- dfm(corpus_essays)

# Trouver les n-grammes les plus fréquents
top_ngrams <- topfeatures(dfm_essays, n = 10)

# Calcul du nombre total de n-grammes uniques par document
train_essays$ngrams_count <- ntoken(dfm_essays)
```

```{r}
library(quanteda)
# Création de tokens et calcul de TTR pour unigrammes (Diversité des n-grammes)
tokens_essays <- tokens(train_essays$text, what = "word")
dfm_essays <- dfm(tokens_essays)
train_essays$TTR <- ntype(dfm_essays) / ntoken(dfm_essays)
print(head(train_essays$TTR))
```

```{r}
library(quanteda)

bigrams_tokens <- tokens_ngrams(tokens_essays, n = 2)
dfm_bigrams <- dfm(bigrams_tokens)

# Identification des bigrammes les plus fréquents dans tout le corpus
top_bigrams <- names(top_ngrams)

# Calcul de la fréquence de chaque top bigramme dans chaque document
for (bigram in top_bigrams) {
  # Création d'un nom de colonne valide pour le dataframe
  col_name <- paste("freq", gsub(" ", "_", bigram), sep = "_")
  
  # Sélection du dfm_bigrams pour le bigramme actuel et somme des fréquences
  selected_dfm <- dfm_select(dfm_bigrams, pattern = bigram)
  
  # Ajout de la somme des fréquences du bigramme au dataframe
  train_essays[[col_name]] <- rowSums(as.matrix(selected_dfm))
}
```

```{r}
# Calcul des poids TF-IDF
tfidf <- dfm_tfidf(dfm_essays)

# Convertir la matrice TF-IDF en format matrice
tfidf_matrix <- as.matrix(tfidf)
# Affichage d'un extrait de la matrice TF-IDF
print(tfidf_matrix[1:5, 1:10])

```

Avant de réduire la dimensionnalité de l’espace, il faut d'abord préciser la nature de la relation : linéaire ou non linéaire afin d'utiliser l'approche convenable, pour se faire analysant la relation entre les différents features.

```{r}
library(ggplot2)

# Création d'un graphique de dispersion pour chaque paire de caractéristiques
ggplot(train_essays, aes(x = longueur_moyenne_phrases, y = Flesch_Score_lisibilité)) +
  geom_point() +
  labs(x = "Longueur moyenne des phrases", y = "Score de lisibilité de Flesch") +
  ggtitle("Longueur moyenne des phrases vs Score de lisibilité de Flesch")

ggplot(train_essays, aes(x = longueur_moyenne_phrases, y = TTR)) +
  geom_point() +
  labs(x = "Longueur moyenne des phrases", y = "TTR (Taux de Type-Token)") +
  ggtitle("Longueur moyenne des phrases vs TTR")

ggplot(train_essays, aes(x = Flesch_Score_lisibilité, y = TTR)) +
  geom_point() +
  labs(x = "Score de lisibilité de Flesch", y = "TTR (Taux de Type-Token)") +
  ggtitle("Score de lisibilité de Flesch vs TTR")


```


```{r}
# Extrait de la matrice de corrélation
correlation_matrix <- cor(train_essays[, c("longueur_moyenne_phrases", "Flesch_Score_lisibilité", "TTR")])

# Affichage des coefficients de corrélation individuels
print(correlation_matrix)
```

-La corrélation entre la 'longueur moyenne des phrases' et 'le score de lisibilité de Flesch' est faible à modérée et négative, indiquant qu'une augmentation de la longueur moyenne des phrases est généralement associée à une diminution du score de lisibilité de Flesch, et vice versa. Cependant, cette corrélation n'est pas très forte, ce qui suggère que d'autres facteurs peuvent également influencer cette relation.
-Il y a une corrélation négative faible entre la 'longueur moyenne des phrases' et le 'TTR' , indiquant qu'une augmentation de la longueur moyenne des phrases est associée à une légère diminution du TTR, et vice versa. Cependant, cette corrélation n'est pas très forte.
-Entre le 'score de lisibilité de Flesch' et le 'TTR', il n'y a pratiquement aucune corrélation linéaire. Cela indique que ces deux caractéristiques sont essentiellement indépendantes l'une de l'autre et que leur variation n'est pas liée de manière linéaire.

```{r}
# Calculer la matrice de corrélation
correlation_matrix <- cor(train_essays[, c("longueur_moyenne_phrases", "Flesch_Score_lisibilité", "TTR")])

# Visualiser la matrice de corrélation sous forme de heatmap
heatmap(correlation_matrix, 
        Colv = NA, Rowv = NA,
        col = colorRampPalette(c("blue", "white", "red"))(100),
        main = "Heatmap de la corrélation entre les caractéristiques",
        xlab = "Caractéristiques", ylab = "Caractéristiques")

```
La visualisation de la matrice de corrélation sous forme de heatmap confirme nos analyses par rapport à la relation entre les caractéristiques.
En conclusion en peut dire que la relation entre les caractéristiques est non linéaires.

```{r}
library(e1071) 
# Séparation des caractéristiques et de la variable cible
X <- train_essays[, c("longueur_moyenne_phrases", "Flesch_Score_lisibilité", "TTR", paste0("freq_", top_bigrams))]
Y <- train_essays$generated

# Entraînement du modèle SVM avec un noyau gaussien (rbf)
model_svm <- svm(X, Y, kernel = "radial", type = "C-classification")

# Afficher un résumé du modèle
summary(model_svm)
```
